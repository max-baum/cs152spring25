{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a1e08ac",
   "metadata": {},
   "source": [
    "# Optimization Assignment\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "1. Clone this repository (or just pull changes if you already have it).\n",
    "2. Start Jupyter (don't forget to activate conda).\n",
    "3. Duplicate this file so that you can still pull changes without merging.\n",
    "4. Complete the \"Things to Implement.\"\n",
    "\n",
    "## Things to Implement\n",
    "\n",
    "1. Momentum\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "m_{t+1} &:= β_m m_t + (1 - β_m) \\nabla_θ L_b(θ_t) \\\\\n",
    "θ_{t+1} &:= θ_t - η m_{t+1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "2. RMSProp\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g_{t+1}^2 &:= β_g g_t^2 + (1 - β_g) (\\nabla_θ L_b(θ_t))^2 \\\\\n",
    "θ_{t+1} &:= θ_t - η \\frac{\\nabla_θ L_b(θ_t)}{\\sqrt{g_{t+1}^2} + ε}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "3. Adam\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "m_{t+1} &:= β_m m_t + (1 - β_m) \\nabla_θ L_b(θ_t) \\\\\n",
    "\\hat m_{t+1} &:= \\frac{m_{t+1}}{1 - β_m^t} \\\\\n",
    "g_{t+1}^2 &:= β_g g_t^2 + (1 - β_g) (\\nabla_θ L_b(θ_t))^2 \\\\\n",
    "\\hat g_{t+1}^2 &:= \\frac{g_{t+1}^2}{1 - β_g^t} \\\\\n",
    "θ_{t+1} &:= θ_t - η \\frac{\\hat m_{t+1}}{\\sqrt{\\hat g_{t+1}^2} + ε}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "A few hints:\n",
    "\n",
    "- Run the code all the way through without any changes and answer the first question on gradescope\n",
    "- Adam combines momentum and RMSProp (and in this case adds a bias correction)\n",
    "- $t$ increments after each update (the actual value of $t$ is only used in Adam)\n",
    "- You'll need to add code in two places\n",
    "    1. At the top of the training cell (to initialize momentums and squared gradients)\n",
    "    2. In the parameter update context manager (where you'll find `param -= ...`)\n",
    "- The documentation for [torch.optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) will give good values for $\\beta_m$ and $\\beta_g$\n",
    "- If momentum performs poorly, then it might be that you are not updating momentum values **in-place**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c955e",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7610e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "jtplot.style(context=\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use some shared space for the data (so that we don't have copies\n",
    "# sitting around everywhere)\n",
    "data_path = \"/data/cs152/cache/pytorch/data\"\n",
    "\n",
    "# Use the GPUs if they are available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using '{device}' device.\")\n",
    "\n",
    "# Model hyperparameters\n",
    "neurons_per_hidden_layer = [13, 17]\n",
    "\n",
    "# Mini-Batch SGD hyperparameters\n",
    "batch_size = 256\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a4ad36",
   "metadata": {},
   "source": [
    "## Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb6a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fmnist_data_loaders(path, batch_size, valid_batch_size=0):\n",
    "\n",
    "    # Data specific transforms\n",
    "    data_mean = (0.2860,)\n",
    "    data_std = (0.3530,)\n",
    "    xforms = Compose([ToTensor(), Normalize(data_mean, data_std)])\n",
    "\n",
    "    # Training data loader\n",
    "    train_dataset = FashionMNIST(root=path, train=True, download=True, transform=xforms)\n",
    "\n",
    "    # Set the batch size to N if batch_size is 0\n",
    "    tbs = len(train_dataset) if batch_size == 0 else batch_size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=tbs, shuffle=True)\n",
    "\n",
    "    # Validation data loader\n",
    "    valid_dataset = FashionMNIST(root=path, train=False, download=True, transform=xforms)\n",
    "\n",
    "    # Set the batch size to N if batch_size is 0\n",
    "    vbs = len(valid_dataset) if valid_batch_size == 0 else valid_batch_size\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=vbs, shuffle=True)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d78186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing normalization constants for Fashion-MNIST (commented out since we only need to do this once)\n",
    "# train_loader, valid_loader = get_fmnist_data_loaders(data_path, 0)\n",
    "# X, _ = next(iter(train_loader))\n",
    "# s, m = torch.std_mean(X)\n",
    "\n",
    "train_loader, valid_loader = get_fmnist_data_loaders(data_path, batch_size)\n",
    "\n",
    "print(\"Training dataset shape   :\", train_loader.dataset.data.shape)\n",
    "print(\"Validation dataset shape :\", valid_loader.dataset.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a58188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot a few images as an example\n",
    "num_to_show = 8\n",
    "images = train_loader.dataset.data[:num_to_show]\n",
    "targets = train_loader.dataset.targets[:num_to_show]\n",
    "labels = [train_loader.dataset.classes[t] for t in targets]\n",
    "\n",
    "fig, axes = plt.subplots(1, num_to_show)\n",
    "\n",
    "for axis, image, label in zip(axes, images, labels):\n",
    "    axis.imshow(image.squeeze(), cmap=\"Greys\")\n",
    "    axis.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "    axis.set_xticks([])\n",
    "    axis.set_yticks([])\n",
    "    axis.set_title(f\"{label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5d50c0",
   "metadata": {},
   "source": [
    "## Create a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # The first \"layer\" just rearranges the Nx28x28 input into Nx784\n",
    "        first_layer = nn.Flatten()\n",
    "\n",
    "        # The hidden layers include:\n",
    "        # 1. a linear component (computing Z) and\n",
    "        # 2. a non-linear comonent (computing A)\n",
    "        hidden_layers = [\n",
    "            nn.Sequential(nn.Linear(nlminus1, nl), nn.ReLU())\n",
    "            for nl, nlminus1 in zip(layer_sizes[1:-1], layer_sizes)\n",
    "        ]\n",
    "\n",
    "        # The output layer must be Linear without an activation. See:\n",
    "        #   https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        output_layer = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n",
    "\n",
    "        # Group all layers into the sequential container\n",
    "        all_layers = [first_layer] + hidden_layers + [output_layer]\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input layer size depends on the dataset\n",
    "nx = train_loader.dataset.data.shape[1:].numel()\n",
    "\n",
    "# The output layer size depends on the dataset\n",
    "ny = len(train_loader.dataset.classes)\n",
    "\n",
    "# Preprend the input and append the output layer sizes\n",
    "layer_sizes = [nx] + neurons_per_hidden_layer + [ny]\n",
    "model = NeuralNetwork(layer_sizes).to(device)\n",
    "\n",
    "summary(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118c4f7f",
   "metadata": {},
   "source": [
    "## Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f98f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying model creation here so that the model is recreated each time the cell is run\n",
    "model = NeuralNetwork(layer_sizes).to(device)\n",
    "\n",
    "t = 0\n",
    "\n",
    "# \n",
    "# TODO: Add your initialization code for momentum, RMSProp, and Adam\n",
    "#\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "# A master bar for fancy output progress\n",
    "mb = master_bar(range(num_epochs))\n",
    "\n",
    "# Information for plots\n",
    "mb.names = [\"Train Loss\", \"Valid Loss\"]\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in mb:\n",
    "\n",
    "    #\n",
    "    # Training\n",
    "    #\n",
    "    model.train()\n",
    "\n",
    "    train_N = len(train_loader.dataset)\n",
    "    num_train_batches = len(train_loader)\n",
    "    train_dataiterator = iter(train_loader)\n",
    "\n",
    "    train_loss_mean = 0\n",
    "\n",
    "    for batch in progress_bar(range(num_train_batches), parent=mb):\n",
    "\n",
    "        # Grab the batch of data and send it to the correct device\n",
    "        train_X, train_Y = next(train_dataiterator)\n",
    "        train_X, train_Y = train_X.to(device), train_Y.to(device)\n",
    "\n",
    "        # Compute the output\n",
    "        train_output = model(train_X)\n",
    "\n",
    "        # Compute loss\n",
    "        train_loss = criterion(train_output, train_Y)\n",
    "\n",
    "        num_in_batch = len(train_X)\n",
    "        tloss = train_loss.item() * num_in_batch / train_N\n",
    "        train_loss_mean += tloss\n",
    "        train_losses.append(train_loss.item())\n",
    "\n",
    "        # Compute partial derivatives\n",
    "        model.zero_grad()\n",
    "        train_loss.backward()\n",
    "\n",
    "        # \n",
    "        # TODO: Add changes for momentum, RMSProp, and Adam\n",
    "        #\n",
    "        t += 1\n",
    "        with torch.no_grad():\n",
    "            # Original gradient descent\n",
    "            for param in model.parameters():\n",
    "                # θ_{t+1} := θ_t - η \\nabla_θ L_b(θ_t)\n",
    "                param -= learning_rate * param.grad\n",
    "                \n",
    "            # Gradient descent with momentum\n",
    "            # for param, momentum in ...\n",
    "            #     m_{t+1} := β_m m_t + (1 - β_m) \\nabla_θ L_b(θ_t)\n",
    "            #     θ_{t+1} := θ_t - η m_{t+1}\n",
    "               \n",
    "            # Gradient descent with RMSProp\n",
    "            # for param, sq_grad in ...\n",
    "            #     g_{t+1}^2 := β_g g_t^2 + (1 - β_g) (\\nabla_θ L_b(θ_t))^2\n",
    "            #     θ_{t+1} := θ_t - η \\frac{\\nabla_θ L_b(θ_t)}{\\sqrt{g_{t+1}^2} + ε}\n",
    "\n",
    "            # Gradient descent with Adam\n",
    "            # for param, momentum, sq_grad in ...\n",
    "            #     m_{t+1} := β_m m_t + (1 - β_m) \\nabla_θ L_b(θ_t)\n",
    "            #     \\hat m_{t+1} := \\frac{m_{t+1}}{1 - β_m^t}\n",
    "            #     g_{t+1}^2 := β_g g_t^2 + (1 - β_g) (\\nabla_θ L_b(θ_t))^2\n",
    "            #     \\hat g_{t+1}^2 := \\frac{g_{t+1}^2}{1 - β_g^t}\n",
    "            #     θ_{t+1} := θ_t - η \\frac{\\hat m_{t+1}}{\\sqrt{\\hat g_{t+1}^2} + ε}\n",
    "\n",
    "    #\n",
    "    # Validation\n",
    "    #\n",
    "    model.eval()\n",
    "\n",
    "    valid_N = len(valid_loader.dataset)\n",
    "    num_valid_batches = len(valid_loader)\n",
    "\n",
    "    valid_loss_mean = 0\n",
    "    valid_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # valid_loader is probably just one large batch, so not using progress bar\n",
    "        for valid_X, valid_Y in valid_loader:\n",
    "\n",
    "            valid_X, valid_Y = valid_X.to(device), valid_Y.to(device)\n",
    "\n",
    "            valid_output = model(valid_X)\n",
    "\n",
    "            valid_loss = criterion(valid_output, valid_Y)\n",
    "\n",
    "            num_in_batch = len(valid_X)\n",
    "            vloss = valid_loss.item() * num_in_batch / valid_N\n",
    "            valid_loss_mean += vloss\n",
    "            valid_losses.append(valid_loss.item())\n",
    "\n",
    "            # Convert network output into predictions (one-hot -> number)\n",
    "            predictions = valid_output.argmax(1)\n",
    "\n",
    "            # Sum up total number that were correct\n",
    "            valid_correct += (predictions == valid_Y).type(torch.float).sum().item()\n",
    "\n",
    "    valid_accuracy = 100 * (valid_correct / valid_N)\n",
    "\n",
    "    # Report information\n",
    "    tloss = f\"Train Loss = {train_loss_mean:.4f}\"\n",
    "    vloss = f\"Valid Loss = {valid_loss_mean:.4f}\"\n",
    "    vaccu = f\"Valid Accuracy = {(valid_accuracy):>0.1f}%\"\n",
    "    mb.write(f\"[{epoch+1:>2}/{num_epochs}] {tloss}; {vloss}; {vaccu}\")\n",
    "\n",
    "    # Update plot data\n",
    "    max_loss = max(max(train_losses), max(valid_losses))\n",
    "    min_loss = min(min(train_losses), min(valid_losses))\n",
    "\n",
    "    x_margin = 0.2\n",
    "    x_bounds = [0 - x_margin, num_epochs + x_margin]\n",
    "\n",
    "    y_margin = 0.1\n",
    "    y_bounds = [min_loss - y_margin, max_loss + y_margin]\n",
    "\n",
    "    valid_Xaxis = torch.linspace(0, epoch + 1, len(train_losses))\n",
    "    valid_xaxis = torch.linspace(1, epoch + 1, len(valid_losses))\n",
    "    graph_data = [[valid_Xaxis, train_losses], [valid_xaxis, valid_losses]]\n",
    "\n",
    "    mb.update_graph(graph_data, x_bounds, y_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971d52b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "434e3f2d58e1385e0adb0e032cbe799909e99708e62ae45506af3a1338bb2ba8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
