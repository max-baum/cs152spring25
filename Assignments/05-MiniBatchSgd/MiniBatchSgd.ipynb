{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a1e08ac",
   "metadata": {},
   "source": [
    "# Mini-Batch SGD Assignment\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "1. Log into [Pomona's Jupyter Hub](https://jupyter.pomona.edu/).\n",
    "2. Clone this repository (or just pull changes if you already have it).\n",
    "3. Start Jupyter (don't forget to use the CS 152 environment).\n",
    "4. Duplicate this file so that you can still pull changes without merging.\n",
    "5. Complete the \"Questions to Answer.\"\n",
    "6. Complete the \"Things to Try.\"\n",
    "\n",
    "## Questions to Answer\n",
    "\n",
    "You will answer these questions on gradescope. Try to answer these with your partner prior to running or altering any code.\n",
    "\n",
    "1. How could you make this code run \"stochastic gradient descent (SGD)\"?\n",
    "\n",
    "    Add an optimizer line after you instantiate the model\n",
    "\n",
    "1. How could you make this code run \"batch gradient descent (BGD)\"?\n",
    "\n",
    "    Add an optimizer line after you instantiate the model\n",
    "\n",
    "1. What is the shape of `train_X`?\n",
    "\n",
    "    60,000 x 28 x 28\n",
    "\n",
    "1. What is the shape of `train_output`?\n",
    "\n",
    "    60,000 x 1 x 1\n",
    "\n",
    "1. What values would you expect to see in the `train_output` tensor?\n",
    "\n",
    "   Predicted letters or indexes to predicted letters\n",
    "\n",
    "1. What is the shape of `train_Y`?\n",
    "\n",
    "   60,000 x 1 x 1 -- i.e. the labels\n",
    "\n",
    "1. What is the shape of the first linear layer's weight matrix?\n",
    "\n",
    "   784 x 13\n",
    "\n",
    "1. How many parameters are in the neural network?\n",
    "\n",
    "   784\n",
    "\n",
    "1. What is the purpose of the `with torch.no_grad()` ([documentation](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad)) context manager?\n",
    "\n",
    "    Disables gradient calculation, such to minimize computational burden. \n",
    "\n",
    "1. How do we compute accuracy? Describe what the code is doing.\n",
    "\n",
    "   The system sums all of the output predictions that match the correct input labels.\n",
    "\n",
    "    ~~~python\n",
    "    # Convert network output into predictions (one-hot -> number)\n",
    "    predictions = valid_output.argmax(1)\n",
    "\n",
    "    # Sum up total number that were correct\n",
    "    valid_correct += (predictions == valid_Y).type(torch.float).sum().item()\n",
    "    ~~~\n",
    "\n",
    "1. What happens when you rerun the training cell for additional epochs without rerunning any other cells?\n",
    "\n",
    "    ?\n",
    "\n",
    "1. What happens if you set the device to \"cpu\"?\n",
    "\n",
    "   We will use the CPU rather than GPU for calculations, which may slow down computation.\n",
    "\n",
    "    ~~~python\n",
    "    # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = \"cpu\"\n",
    "    ~~~\n",
    "\n",
    "## Things to Try\n",
    "\n",
    "1. Change the hidden layer activation functions to sigmoid. What were the results?\n",
    "\n",
    "1. Change the hidden layer activation functions to [something else](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity). What were the results?\n",
    "\n",
    "1. Change the hidden layer activation functions to `nn.Identify`. What were the results?\n",
    "\n",
    "1. (Optional) Try adding a [dropout layer](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout) after each activation function. What were the results?\n",
    "\n",
    "1. (Optional) Try changing the dataset to either [KMNIST](https://pytorch.org/vision/0.11/datasets.html#kmnist) or [Fashion-MNIST](https://pytorch.org/vision/0.11/datasets.html#fashion-mnist). What were the results?\n",
    "\n",
    "1. (Optional) Try out the **inference** process.\n",
    "\n",
    "    1. Save the model. \n",
    "    \n",
    "    ~~~python\n",
    "    # All training code above\n",
    "    model_filename = \"A05Model.pth\"\n",
    "    torch.save(model.state_dict(), model_filename)\n",
    "    ~~~\n",
    "    \n",
    "    1. Create a new notebook.\n",
    "    \n",
    "    1. Load the saved model.\n",
    "    \n",
    "    ~~~python\n",
    "    # Need to bring over some code from the training file to make this work\n",
    "    model = NeuralNetwork(layer_sizes)\n",
    "    model.load_state_dict(torch.load(model_filename))\n",
    "    model.eval()\n",
    "    \n",
    "    # Index of a validation example\n",
    "    i = 0\n",
    "\n",
    "    # Example input and output\n",
    "    x, y = valid_loader.dataset[i][0], valid_loader.dataset[i][1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(x)\n",
    "        prediction = output[0].argmax(0)\n",
    "        print(f\"Prediction : {prediction}\")\n",
    "        print(f\"Target     : {y}\")\n",
    "    ~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f241528a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7610e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "jtplot.style(context=\"paper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c955e",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b77abda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'cuda' device.\n"
     ]
    }
   ],
   "source": [
    "# Let's store the MNIST dataset in the root of your user directory\n",
    "# You can delete it when you are done with this notebook\n",
    "data_path = \"~/data\"\n",
    "\n",
    "# Use the GPUs if they are available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using '{device}' device.\")\n",
    "\n",
    "# Model hyperparameters\n",
    "neurons_per_layer = [13, 17]\n",
    "\n",
    "# Mini-Batch SGD hyperparameters\n",
    "batch_size = 256\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "activation_function = nn.ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a4ad36",
   "metadata": {},
   "source": [
    "## Prepare the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cb6a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_data_loaders(path, batch_size, valid_batch_size=0):\n",
    "\n",
    "    # MNIST specific transforms\n",
    "    mnist_mean = (0.1307,)\n",
    "    mnist_std = (0.3081,)\n",
    "    mnist_xforms = Compose([ToTensor(), Normalize(mnist_mean, mnist_std)])\n",
    "\n",
    "    # Training data loader\n",
    "    train_dataset = MNIST(root=path, train=True, download=True, transform=mnist_xforms)\n",
    "\n",
    "    # Set the batch size to N if batch_size is 0\n",
    "    tbs = len(train_dataset) if batch_size == 0 else batch_size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=tbs, shuffle=True)\n",
    "\n",
    "    # Validation data loader\n",
    "    valid_dataset = MNIST(root=path, train=False, download=True, transform=mnist_xforms)\n",
    "\n",
    "    # Set the batch size to N if batch_size is 0\n",
    "    vbs = len(valid_dataset) if valid_batch_size == 0 else valid_batch_size\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=vbs, shuffle=True)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35d78186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /home/miba2020/data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:01<00:00, 6.70MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/miba2020/data/MNIST/raw/train-images-idx3-ubyte.gz to /home/miba2020/data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /home/miba2020/data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 416kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/miba2020/data/MNIST/raw/train-labels-idx1-ubyte.gz to /home/miba2020/data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /home/miba2020/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.20MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/miba2020/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/miba2020/data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /home/miba2020/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 14.7MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/miba2020/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/miba2020/data/MNIST/raw\n",
      "\n",
      "Training dataset shape   : torch.Size([60000, 28, 28])\n",
      "Validation dataset shape : torch.Size([10000, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader = get_mnist_data_loaders(data_path, batch_size)\n",
    "\n",
    "print(\"Training dataset shape   :\", train_loader.dataset.data.shape)\n",
    "print(\"Validation dataset shape :\", valid_loader.dataset.data.shape)\n",
    "\n",
    "# Notice that each example is 28x28. These are images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a58188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot a few images as an example\n",
    "num_to_show = 8\n",
    "images = train_loader.dataset.data[:num_to_show]\n",
    "labels = train_loader.dataset.targets[:num_to_show]\n",
    "\n",
    "fig, axes = plt.subplots(1, num_to_show)\n",
    "\n",
    "for axis, image, label in zip(axes, images, labels):\n",
    "    axis.imshow(image.squeeze(), cmap=\"Greys\")\n",
    "    axis.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "    axis.set_xticks([])\n",
    "    axis.set_yticks([])\n",
    "    axis.set_title(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the underlying data for a single image\n",
    "train_loader.dataset.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d5b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can almost make out the \"5\" in the output above\n",
    "# Let's make it a bit more clear\n",
    "image = train_loader.dataset.data[0]\n",
    "image_df = pd.DataFrame(image.squeeze().numpy())\n",
    "image_df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5d50c0",
   "metadata": {},
   "source": [
    "## Create a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, act_func):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # The first \"layer\" just rearranges the Nx28x28 input into Nx784\n",
    "        first_layer = nn.Flatten()\n",
    "\n",
    "        # The hidden layers include:\n",
    "        # 1. a linear component (computing Z) and\n",
    "        # 2. a non-linear comonent (computing A)\n",
    "        hidden_layers = [\n",
    "            nn.Sequential(nn.Linear(nlminus1, nl), act_func())\n",
    "            for nl, nlminus1 in zip(layer_sizes[1:-1], layer_sizes)\n",
    "        ]\n",
    "\n",
    "        # The output layer must be Linear WITHOUT an activation. See:\n",
    "        #   https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        output_layer = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n",
    "\n",
    "        # Group all layers into the sequential container\n",
    "        all_layers = [first_layer] + hidden_layers + [output_layer]\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Since we've wrapped all layers in nn.Sequential, we just have to\n",
    "        # call one method and not manually pass the input forward\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input layer size depends on the dataset\n",
    "n0 = train_loader.dataset.data.shape[1:].numel()\n",
    "\n",
    "# The output layer size depends on the dataset\n",
    "nL = len(train_loader.dataset.classes)\n",
    "\n",
    "# Preprend the input and append the output layer sizes\n",
    "layer_sizes = [n0] + neurons_per_layer + [nL]\n",
    "model = NeuralNetwork(layer_sizes, activation_function).to(device)\n",
    "\n",
    "summary(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118c4f7f",
   "metadata": {},
   "source": [
    "## Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f98f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information for plots\n",
    "fig, ax = plt.subplots()\n",
    "dh = display(fig, display_id=True)\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in tqdm_notebook(range(num_epochs), desc=\"Training epochs\"):\n",
    "\n",
    "    #\n",
    "    # Training\n",
    "    #\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    train_N = len(train_loader.dataset)\n",
    "    num_train_batches = len(train_loader)\n",
    "    train_dataiterator = iter(train_loader)\n",
    "\n",
    "    train_loss_mean = 0\n",
    "\n",
    "    # for batch in progress_bar(range(num_train_batches), parent=mb):\n",
    "    for batch in tqdm_notebook(range(num_train_batches), desc=\"Training batches\", leave=False):\n",
    "\n",
    "        # Grab the batch of data and send it to the correct device\n",
    "        train_X, train_Y = next(train_dataiterator)\n",
    "        train_X, train_Y = train_X.to(device), train_Y.to(device)\n",
    "\n",
    "        # Compute the output\n",
    "        train_output = model(train_X)\n",
    "\n",
    "        # Compute loss\n",
    "        train_loss = criterion(train_output, train_Y)\n",
    "\n",
    "        num_in_batch = len(train_X)\n",
    "        tloss = train_loss.item() * num_in_batch / train_N\n",
    "        train_loss_mean += tloss\n",
    "        train_losses.append(train_loss.item())\n",
    "\n",
    "        # Compute partial derivatives\n",
    "        model.zero_grad()\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "\n",
    "    #\n",
    "    # Validation\n",
    "    #\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    valid_N = len(valid_loader.dataset)\n",
    "    num_valid_batches = len(valid_loader)\n",
    "\n",
    "    valid_loss_mean = 0\n",
    "    valid_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # valid_loader is probably just one large batch, so not using progress bar\n",
    "        for valid_X, valid_Y in valid_loader:\n",
    "\n",
    "            valid_X, valid_Y = valid_X.to(device), valid_Y.to(device)\n",
    "\n",
    "            valid_output = model(valid_X)\n",
    "\n",
    "            valid_loss = criterion(valid_output, valid_Y)\n",
    "\n",
    "            num_in_batch = len(valid_X)\n",
    "            vloss = valid_loss.item() * num_in_batch / valid_N\n",
    "            valid_loss_mean += vloss\n",
    "            valid_losses.append(valid_loss.item())\n",
    "\n",
    "            # Convert network output into predictions (one-hot -> number)\n",
    "            predictions = valid_output.argmax(1)\n",
    "\n",
    "            # Sum up total number that were correct\n",
    "            valid_correct += (predictions == valid_Y).type(torch.float).sum().item()\n",
    "\n",
    "    valid_accuracy = 100 * (valid_correct / valid_N)\n",
    "\n",
    "    # \n",
    "    # Report information\n",
    "    # \n",
    "    \n",
    "    tloss = f\"Train Loss = {train_loss_mean:.4f}\"\n",
    "    vloss = f\"Valid Loss = {valid_loss_mean:.4f}\"\n",
    "    vaccu = f\"Valid Accuracy = {(valid_accuracy):>0.1f}%\"\n",
    "    print(f\"[{epoch+1:>2}/{num_epochs}] {tloss}; {vloss}; {vaccu}\")\n",
    "\n",
    "    # \n",
    "    # Update plot\n",
    "    # \n",
    "    \n",
    "    max_loss = max(max(train_losses), max(valid_losses))\n",
    "    min_loss = min(min(train_losses), min(valid_losses))\n",
    "    \n",
    "    x_margin = 0.2\n",
    "    x_bounds = [0 - x_margin, num_epochs + x_margin]\n",
    "\n",
    "    y_margin = 0.1\n",
    "    y_bounds = [min_loss - y_margin, max_loss + y_margin]\n",
    "\n",
    "    train_xaxis = torch.linspace(0, epoch + 1, len(train_losses))\n",
    "    valid_xaxis = torch.linspace(1, epoch + 1, len(valid_losses))\n",
    "    graph_data = [[train_xaxis, train_losses], [valid_xaxis, valid_losses]]\n",
    "\n",
    "    ax.clear()\n",
    "    \n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "\n",
    "    ax.set_xlim(x_bounds)\n",
    "    ax.set_ylim(y_bounds)\n",
    "\n",
    "    ax.plot(train_xaxis, train_losses, label=\"Train\")\n",
    "    ax.plot(valid_xaxis, valid_losses, label=\"Valid\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "\n",
    "    dh.update(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30352f3c-b9be-420f-9ad9-278240270584",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python3 (cs152)",
   "language": "python",
   "name": "cs152"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
